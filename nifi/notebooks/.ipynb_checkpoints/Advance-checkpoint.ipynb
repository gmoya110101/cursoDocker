{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8cfdd0-5bfb-4b2b-a4e3-11deccc681f6",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: Funciones avanzadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bf8ce-12b6-4e67-afe3-49d696092c04",
   "metadata": {},
   "source": [
    "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e44a81-aabd-4deb-9792-33708de633a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728eafbf-a24a-4b78-aff1-625e25358796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b74bbc93-cdac-4897-8bba-b7fe0f01e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crea la sesión SPARK y le indico que use el driver de postgres\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/postgresql-42.7.4.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acef7e8d-3a71-4023-81c9-1ee06444d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Save as HIVE tables.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c998493-deac-4e3b-b029-4a00bd3faed3",
   "metadata": {},
   "source": [
    "## BroadCast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c8a11-fa82-4ade-b052-e722ee0bd01c",
   "metadata": {},
   "source": [
    "El tamaño de la tabla de difusión es de 10 MB. Sin embargo, podemos cambiar el umbral hasta 8GB según la documentación oficial de Spark 2.3.\n",
    "\n",
    "* Podemos verificar el tamaño de la tabla de transmisión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ece292d-dc9d-43c4-8c17-3ae43a1e9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 50.0 MB.\n"
     ]
    }
   ],
   "source": [
    "# Yo necesité cortar la cadena\n",
    "# porque estaba en binario y me daba problemas al momento de castear a int\n",
    "\n",
    "bytes = str(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")).split('b',0)[0]\n",
    "size = int(bytes) / (1024 ** 2)\n",
    "print(\"Default size of broadcast table is {0} MB.\".format(size))\n",
    "# Default 10.0 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3c7c9-8df5-4fb3-b67e-b62fcce16a3b",
   "metadata": {},
   "source": [
    "* Podemos establecer el tamaño de la tabla de transmisión para que diga 50 MB de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "494c0866-867e-4b0a-8260-3b87ba1909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa1094-216d-4aaa-9515-b2caa4090850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considere que necesitamos unir 2 Dataframes.\n",
    "# small_df: DataFrame pequeño que puede caber en la memoria y es más pequeño que el umbral especificado.\n",
    "# big_df: DataFrame grande que debe unirse con DataFrame pequeño.\n",
    "\n",
    "join_df = big_df.join(broadcast(small_df), big_df[\"id\"] == small_df[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb4fea-a3ca-4855-9ac8-c895a97e1587",
   "metadata": {},
   "source": [
    "## Almacenamiento en caché\n",
    "Podemos usar la función de caché / persistencia para mantener el marco de datos en la memoria. Puede mejorar significativamente el rendimiento de su aplicación Spark si almacenamos en caché los datos que necesitamos usar con mucha frecuencia en nuestra aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dda3d1c-9bda-4db9-b3aa-0712091b0548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: True\n",
      "Disk used: True\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()\n",
    "print(f\"Memory used: {df.storageLevel.useMemory}\")\n",
    "print(f\"Disk used: {df.storageLevel.useDisk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52185c-5ad0-434f-886b-c54392b317de",
   "metadata": {},
   "source": [
    "Cuando usamos la función de caché, usará el nivel de almacenamiento como Memory_Only hasta Spark 2.0.2. Desde Spark 2.1.x es Memory_and_DISK.\r\n",
    "\r\n",
    "Sin embargo, si necesitamos especificar los distintos niveles de almacenamiento disponibles, podemos usar el método persist( ). Por ejemplo, si necesitamos mantener los datos solo en la memoria, podemos usar el siguiente fragmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d95a96b8-9ff5-4c70-baf6-8a3d8616db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed60b6c-d5cf-441c-9df2-841b1827cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: False\n",
      "Disk used: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used: {deptdf.storageLevel.useMemory}\")\n",
    "print(f\"Disk used: {deptdf.storageLevel.useDisk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35da83e3-edf0-4be7-abf9-9e1ff54d1515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: True\n",
      "Disk used: True\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(f\"Memory used: {df.storageLevel.useMemory}\")\n",
    "print(f\"Disk used: {df.storageLevel.useDisk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbcbf3-6394-4470-93d0-0b73874001d5",
   "metadata": {},
   "source": [
    "### No persistir\n",
    "También es importante eliminar la memoria caché de los datos cuando ya no sean necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d783b0dd-20dd-4841-a273-be44165faf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Borra una en específico\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d5e6ffb-2cf3-443b-a632-5775b1e41b72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Borra todas alv ☠️\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msqlContext\u001b[49m\u001b[38;5;241m.\u001b[39mclearCache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "#Borra todas alv ☠️\n",
    "sqlContext.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
