{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d3fc2a-a89e-4d90-8be3-9cd9aef0d57e",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: SQL/DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aab024-41ac-4002-9884-3a7fcd7ef406",
   "metadata": {},
   "source": [
    "**Spark SQLtrabaja con DataFrames**. Un DataFrame es una **representación relacional de los datos**. Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos.\r\n",
    "\r\n",
    "Los DataFrames son similares a las tablas relacionales o DataFrames en Python / R auqnue con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario. Hay varias formas de crear DataFrames a partir de colecciones, tablas HIVE, tablas relacionales y RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528243bf-1326-42e5-86e1-e9980669840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e178379-8adb-43e2-9cd1-ce8dc5f550a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229e05e2-aa43-48e5-b83c-dceb969447bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars\", \"/postgresql-42.7.4.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003b62c3-2902-4085-94be-69350fc14ccd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (8, \"HHH\", \"dept3\", 3700),\n",
    "    (9, \"III\", \"dept3\", 4500),\n",
    "    (10, \"JJJ\", \"dept5\", 3400)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "\n",
    "       ]\n",
    "\n",
    "# Crea un df\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0c8aa2-9cab-458a-8db2-ca47450b0552",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `tbl_name` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [tbl_name], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtbl_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `tbl_name` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [tbl_name], [], false\n"
     ]
    }
   ],
   "source": [
    "# Crea una tabla a partir de una tabla hive\n",
    "df = spark.table(\"tbl_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d906ad5-891d-478f-939d-f89984ae2d40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7bd7c7-5caf-45fd-8c50-9ce6d031308a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|   id|          name|\n",
      "+-----+--------------+\n",
      "|dept1|Department - 1|\n",
      "|dept2|Department - 2|\n",
      "|dept3|Department - 3|\n",
      "|dept4|Department - 4|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Muestra el contenido de un df\n",
    "deptdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395620bd-625e-4ed1-9745-99044c1cc7a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cuenta total de registros\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8535db07-3eb6-4b1f-bee1-a6d93f5d9d4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'dept', 'salary']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista las columnas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e03f6b9-5132-413d-8921-8c3fbabd23a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('dept', 'string'),\n",
       " ('salary', 'bigint')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el tipo de dato de cada columna\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7972075-2de8-41d7-a94d-7595dcf21735",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('dept', StringType(), True), StructField('salary', LongType(), True)])\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Muestra como spark guarda los dataframes\n",
    "print(df.schema)\n",
    "\n",
    "print()\n",
    "# Hace lo mismo pero de una forma más ordenada\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "505addb8-22f9-4b62-b9bc-95a907bf5b1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| AAA|\n",
      "|  2| BBB|\n",
      "|  3| CCC|\n",
      "|  4| DDD|\n",
      "|  5| EEE|\n",
      "|  6| FFF|\n",
      "|  7| GGG|\n",
      "|  8| HHH|\n",
      "|  9| III|\n",
      "| 10| JJJ|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtra por columnas\n",
    "\n",
    "df.select(\"id\",\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a1390b7-5ea7-41d2-b6bb-f6cee9b5864a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  5| EEE|dept2|  8000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Todos sirven para filtar\n",
    "\n",
    "df.filter(df[\"name\"] == 'AAA').show()\n",
    "\n",
    "df.filter(df.id ==1 ).show()\n",
    "\n",
    "df.filter(col(\"id\")  == 1).show()\n",
    "\n",
    "# Con este puedes hacerlo igualito al where de SQL\n",
    "df.filter(\"salary = 8000 or id = 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2d1009c-973c-451a-852e-8e2a28377620",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "| BBB|dept1|  1100|\n",
      "| CCC|dept1|  3000|\n",
      "| DDD|dept1|  1500|\n",
      "| EEE|dept2|  8000|\n",
      "| FFF|dept2|  7200|\n",
      "| GGG|dept3|  7100|\n",
      "| HHH|dept3|  3700|\n",
      "| III|dept3|  4500|\n",
      "| JJJ|dept5|  3400|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45b930c3-7c34-4b66-b8b7-129c590a1414",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+------+\n",
      "| dept|count|  sum| max| min|   avg|\n",
      "+-----+-----+-----+----+----+------+\n",
      "|dept1|    4| 6600|3000|1000|1650.0|\n",
      "|dept2|    2|15200|8000|7200|7600.0|\n",
      "|dept3|    3|15300|7100|3700|5100.0|\n",
      "|dept5|    1| 3400|3400|3400|3400.0|\n",
      "+-----+-----+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Así hago agregaciones a los datos\n",
    "# * Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados.\n",
    "(df.groupBy(\"dept\")\n",
    "     .agg(\n",
    "         count(\"salary\").alias(\"count\"),\n",
    "         sum(\"salary\").alias(\"sum\"),\n",
    "         max(\"salary\").alias(\"max\"),\n",
    "         min(\"salary\").alias(\"min\"),\n",
    "         avg(\"salary\").alias(\"avg\")\n",
    "     ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eabb3989-4620-4457-8343-2209eefc695b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "| BBB|dept1|  1100|\n",
      "| DDD|dept1|  1500|\n",
      "| CCC|dept1|  3000|\n",
      "| JJJ|dept5|  3400|\n",
      "| HHH|dept3|  3700|\n",
      "| III|dept3|  4500|\n",
      "| GGG|dept3|  7100|\n",
      "| FFF|dept2|  7200|\n",
      "| EEE|dept2|  8000|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| EEE|dept2|  8000|\n",
      "| FFF|dept2|  7200|\n",
      "| GGG|dept3|  7100|\n",
      "| III|dept3|  4500|\n",
      "| HHH|dept3|  3700|\n",
      "| JJJ|dept5|  3400|\n",
      "| CCC|dept1|  3000|\n",
      "| DDD|dept1|  1500|\n",
      "| BBB|dept1|  1100|\n",
      "| AAA|dept1|  1000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordena los datos de manera ascendente\n",
    "df.sort(\"salary\").show()\n",
    "\n",
    "# Ordena los datos de manera ascendente\n",
    "df.sort(desc(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6278bf0-9d20-422f-9ce6-326eeb34f154",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+------+\n",
      "|name| dept|salary|bonus|  neto|\n",
      "+----+-----+------+-----+------+\n",
      "| AAA|dept1|  1000|100.0|1100.0|\n",
      "| BBB|dept1|  1100|110.0|1210.0|\n",
      "| CCC|dept1|  3000|300.0|3300.0|\n",
      "| DDD|dept1|  1500|150.0|1650.0|\n",
      "| EEE|dept2|  8000|800.0|8800.0|\n",
      "| FFF|dept2|  7200|720.0|7920.0|\n",
      "| GGG|dept3|  7100|710.0|7810.0|\n",
      "| HHH|dept3|  3700|370.0|4070.0|\n",
      "| III|dept3|  4500|450.0|4950.0|\n",
      "| JJJ|dept5|  3400|340.0|3740.0|\n",
      "+----+-----+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Columnas derivadas, esas columnas son virtuales, \n",
    "# no se agregan al DF\n",
    "# Pero veo que puedo usarlas en tiempo de ejecución y seguir trabajando con ellas\n",
    "df.withColumn(\"bonus\", col(\"salary\")*.1).withColumn(\"neto\",col(\"bonus\")+col(\"salary\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e9fb9-a165-4f18-894b-30c3060fe335",
   "metadata": {},
   "source": [
    "## JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0af0e59-94f2-4f84-b380-dd1339a4337f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+--------------+\n",
      "|name| dept|salary|   id|          name|\n",
      "+----+-----+------+-----+--------------+\n",
      "| AAA|dept1|  1000|dept1|Department - 1|\n",
      "| BBB|dept1|  1100|dept1|Department - 1|\n",
      "| CCC|dept1|  3000|dept1|Department - 1|\n",
      "| DDD|dept1|  1500|dept1|Department - 1|\n",
      "| EEE|dept2|  8000|dept2|Department - 2|\n",
      "| FFF|dept2|  7200|dept2|Department - 2|\n",
      "| GGG|dept3|  7100|dept3|Department - 3|\n",
      "| HHH|dept3|  3700|dept3|Department - 3|\n",
      "| III|dept3|  4500|dept3|Department - 3|\n",
      "+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INNER JOIN\n",
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50a1ccd2-0ec7-4967-b525-3d0a9f890d90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+--------------+\n",
      "|name| dept|salary|   id|          name|\n",
      "+----+-----+------+-----+--------------+\n",
      "| AAA|dept1|  1000|dept1|Department - 1|\n",
      "| BBB|dept1|  1100|dept1|Department - 1|\n",
      "| CCC|dept1|  3000|dept1|Department - 1|\n",
      "| DDD|dept1|  1500|dept1|Department - 1|\n",
      "| EEE|dept2|  8000|dept2|Department - 2|\n",
      "| FFF|dept2|  7200|dept2|Department - 2|\n",
      "| GGG|dept3|  7100|dept3|Department - 3|\n",
      "| HHH|dept3|  3700|dept3|Department - 3|\n",
      "| III|dept3|  4500|dept3|Department - 3|\n",
      "| JJJ|dept5|  3400| NULL|          NULL|\n",
      "+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEFT OUTER\n",
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "faea809f-b71c-4b5d-b10a-89356019b943",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+--------------+\n",
      "|name| dept|salary|   id|          name|\n",
      "+----+-----+------+-----+--------------+\n",
      "| AAA|dept1|  1000|dept1|Department - 1|\n",
      "| BBB|dept1|  1100|dept1|Department - 1|\n",
      "| CCC|dept1|  3000|dept1|Department - 1|\n",
      "| DDD|dept1|  1500|dept1|Department - 1|\n",
      "| EEE|dept2|  8000|dept2|Department - 2|\n",
      "| FFF|dept2|  7200|dept2|Department - 2|\n",
      "| GGG|dept3|  7100|dept3|Department - 3|\n",
      "| HHH|dept3|  3700|dept3|Department - 3|\n",
      "| III|dept3|  4500|dept3|Department - 3|\n",
      "+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RIGHT OUTER\n",
    "df.join(deptdf,df[\"dept\"] == deptdf[\"id\"], \"right_outer\").filter( df[\"name\"] != \"NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81742221-b5cd-4ec1-8e84-5b2c38a25de3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|NULL|NULL| NULL|  NULL|dept4|Department - 4|\n",
      "|  10| JJJ|dept5|  3400| NULL|          NULL|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FULL OUTER\n",
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fd9a3-d1f0-424c-82cf-3e07d449ef00",
   "metadata": {},
   "source": [
    "## Consultas SQL\n",
    "+ Se deben crear vistas temporales de los DF si quieremos usar\n",
    "SQL para poder manipularlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6897de26-31d3-4a3a-a107-ab5e6325bedc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM temp_table LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a8a3a-648b-48f3-836f-b0f20d6a618b",
   "metadata": {},
   "source": [
    "## Crea un DF a partir de una tabla relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "169b9687-567d-4201-aab2-477ecbaafdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_properties = {\n",
    "    \"user\": \"kafka_dev\",\n",
    "    \"password\": \"kafka_dev123\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "755dae0a-01b1-4f3b-a447-eefe3d60751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASÍ TRANSFORMO UNA TABLA DE DB EN UN DF\n",
    "relational_df = spark.read.jdbc(url = 'jdbc:postgresql://postgres:5432/kafka', \n",
    "                                table = 'test.users',\n",
    "                                properties = jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3b09571-aaa8-48d6-81e7-be8b6b911858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|         insdatetime|\n",
      "+---+-------+---+--------------------+\n",
      "|  1|Gerardo| 23|2024-08-29 17:28:...|\n",
      "|  3|     Lu| 23|2024-08-29 17:28:...|\n",
      "|  4|  Naomi| 23|2024-08-29 17:28:...|\n",
      "|  8| Carlos| 17|2024-08-29 17:28:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relational_df.filter(relational_df['age'] < 24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5731b500-529e-4271-998e-733f5abe7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASÍ GUARDO UN DATAFRAME EN UNA DB\n",
    "deptdf.write.jdbc(url = 'jdbc:postgresql://postgres:5432/kafka', table='test.departments',properties=jdbc_properties, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebf482-d00b-44a3-93f8-533198cde2cc",
   "metadata": {},
   "source": [
    "### Guardar el DataFrame como una tabla externa HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e2621-2300-41ad-b541-0abb9cedc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3407cdb-c157-48ac-abee-6d26fd99c947",
   "metadata": {},
   "source": [
    "### Crea un DataFrame a partir de un archivo CSV\n",
    "* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f7127-841f-4f40-9e99-6341ed98f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a48eb6-1029-4bc5-b709-9a6532226a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guardar un DataFrame como un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ffe2d-ee9b-4109-b75e-7ab2d56fa30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
